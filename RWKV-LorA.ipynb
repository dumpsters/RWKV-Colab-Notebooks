{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hq7DhSMBj9De"
      },
      "source": [
        "[Updates](https://github.com/dumpsters/RWKV-Colab-Notebooks)\n",
        "---\n",
        "\n",
        "Thanks to:\n",
        "\n",
        "[RWKV-notebooks](https://github.com/resloved/RWKV-notebooks)\n",
        "\n",
        "[RWKV-LM](https://github.com/BlinkDL/RWKV-LM)\n",
        "\n",
        "[RWKV-LM-LorA](https://github.com/Blealtan/RWKV-LM-LoRA)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cs3G0Pu-il2a"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqAJeElCl9cm"
      },
      "outputs": [],
      "source": [
        "#@title Create and mount folders { display-mode: \"form\" }\n",
        "#@markdown `model_dir` is relative to your Google Drive's root folder\n",
        "\n",
        "drive_dir = '/content/drive'\n",
        "model_dir = 'AI/RWKV' #@param {type:\"string\"}\n",
        "tuned_dir = 'tuned-LorA' #@param {type:\"string\"}\n",
        "dataset_name = 'trainable' #@param {type:\"string\"}\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(drive_dir, force_remount=True)\n",
        "\n",
        "output_path = f\"{drive_dir}/MyDrive/{model_dir}\"\n",
        "checkpoint_dir = f\"{output_path}/{tuned_dir}\"\n",
        "dataset_dir = f\"{checkpoint_dir}/dataset\"\n",
        "dataset_raw = f\"{dataset_dir}/{dataset_name}\"\n",
        "dataset_file = f\"{dataset_raw}_text_document\" # hardcoded suffix somwhere I forgot\n",
        "\n",
        "from os import makedirs\n",
        "makedirs(f\"{checkpoint_dir}\", exist_ok=True)\n",
        "makedirs(f\"{dataset_dir}/{dataset_name}\", exist_ok=True)\n",
        "makedirs(f\"{output_path}/base_models/\", exist_ok=True)\n",
        "\n",
        "#!nvidia-smi\n",
        "\n",
        "print(f\"LorA checkpoints will be saved to {checkpoint_dir}\")\n",
        "print(f\"Place your jsonl dataset files in {dataset_dir}/{dataset_name} if you want to tokenize\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WwCaLe3inUs1"
      },
      "outputs": [],
      "source": [
        "#@title Clone RWKV-LorA and the tokenizer we will use\n",
        "!git clone https://github.com/Blealtan/RWKV-LM-LoRA.git RWKV-LorA\n",
        "!git clone https://github.com/EleutherAI/gpt-neox.git GPT-NeoX\n",
        "\n",
        "# RWKV\n",
        "!pip install deepspeed==0.8.3 --quiet\n",
        "!pip install pytorch-lightning==1.9.1 --quiet\n",
        "!pip install torch transformers wandb ninja --quiet\n",
        "\n",
        "# NeoX\n",
        "!pip install -r ./GPT-NeoX/requirements/requirements.txt --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iupLcT3SXPmG"
      },
      "source": [
        "# Load models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGjfWAugXSqu"
      },
      "outputs": [],
      "source": [
        "#@title Load checkpoint or start a new { display-mode: \"form\" }\n",
        "#@markdown Check `restore_checkpoint` to load a checkpoint from your `tuned_dir`\n",
        "%cd /content/\n",
        "base_model_name = \"RWKV-4-Pile-430M\" #@param [\"RWKV-4-Raven-1B5-v8\", \"RWKV-4-Pile-430M\", \"RWKV-4-Pile-169M\"]\n",
        "restore_checkpoint = False #@param {type:\"boolean\"}\n",
        "\n",
        "model_filename = f\"{base_model_name}.pth\"\n",
        "\n",
        "from huggingface_hub import hf_hub_url\n",
        "if base_model_name == \"RWKV-4-Pile-169M\":\n",
        "    base_model_repo = f\"BlinkDL/rwkv-4-pile-169m\"\n",
        "    model_url = hf_hub_url(repo_id=f\"{base_model_repo}\", filename=\"RWKV-4-Pile-169M-20220807-8023.pth\")\n",
        "    n_layer = 12\n",
        "    n_embd = 768\n",
        "if base_model_name == \"RWKV-4-Pile-430M\":\n",
        "    base_model_repo = f\"BlinkDL/rwkv-4-pile-430m\"\n",
        "    model_url = hf_hub_url(repo_id=f\"{base_model_repo}\", filename=\"RWKV-4-Pile-430M-20220808-8066.pth\")\n",
        "    n_layer = 24\n",
        "    n_embd = 1024\n",
        "if base_model_name == \"RWKV-4-Raven-1B5-v8\":\n",
        "    base_model_repo = f\"BlinkDL/rwkv-4-raven\"\n",
        "    model_url = hf_hub_url(repo_id=f\"{base_model_repo}\", filename=\"RWKV-4-Raven-1B5-v8-Eng-20230408-ctx4096.pth\")\n",
        "    n_layer = 24\n",
        "    n_embd = 2048\n",
        "\n",
        "import os\n",
        "if os.path.isfile(model_filename) == False:\n",
        "  !curl -L $model_url -o $model_filename\n",
        "\n",
        "from glob import glob\n",
        "model_path = glob(f\"/content/{base_model_name}/*.pth\")[0]\n",
        "if restore_checkpoint == True:\n",
        "  checkpoint_path = glob(f\"{checkpoint_dir}/*.pth\")[0]\n",
        "  print(f\"Using {checkpoint_path} as LorA checkpoint\")\n",
        "  print(f\"Set epoch_begin manually to your last epoch +1\")\n",
        "print(f\"Using {model_path} as base\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oh8XRqBHbzlM"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6J9_Hjywb2Uu"
      },
      "outputs": [],
      "source": [
        "#@title Create dataset out of jsonl files { display-mode: \"form\" }\n",
        "#@markdown In case you mounted your drive before copying the jsonl files over, re-run the setup part again before running this.\n",
        "use_ftfy = False #@param {type:\"boolean\"}\n",
        "\n",
        "%cd /content/\n",
        "print(f\"Tokenizing jsonl files in {dataset_raw}\")\n",
        "\n",
        "if use_ftfy:\n",
        "  !python ./GPT-NeoX/tools/preprocess_data.py \\\n",
        "  --input $dataset_raw \\\n",
        "  --tokenizer-type HFTokenizer \\\n",
        "  --vocab-file ./RWKV-LorA/RWKV-v4neo/20B_tokenizer.json \\\n",
        "  --output-prefix $dataset_raw \\\n",
        "  --append-eod \\\n",
        "  --dataset-impl mmap \\\n",
        "  --ftfy\n",
        "else:\n",
        "  !python ./GPT-NeoX/tools/preprocess_data.py \\\n",
        "  --input $dataset_raw \\\n",
        "  --tokenizer-type HFTokenizer \\\n",
        "  --vocab-file ./RWKV-LorA/RWKV-v4neo/20B_tokenizer.json \\\n",
        "  --output-prefix $dataset_raw \\\n",
        "  --append-eod \\\n",
        "  --dataset-impl mmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05K-elkHdW_j"
      },
      "outputs": [],
      "source": [
        "#@title Finetune { display-mode: \"form\" }\n",
        "#@markdown Add `MAX_JOBS=1` in front of the python command if you're tuning 1B5 or above.\n",
        "epoch_count = 1000 #@param {type:\"integer\"}\n",
        "epoch_begin = 0 #@param {type:\"integer\"}\n",
        "epoch_steps = 500 #@param {type:\"integer\"}\n",
        "epoch_save_frequency = 1 #@param {type:\"integer\"}\n",
        "micro_bsz =  10 #@param {type:\"integer\"} \n",
        "ctx_len = 1024 #@param {type:\"integer\"}\n",
        "precision = 'bf16' #@param ['fp16', 'fp32', 'tf32', 'bf16'] {type:\"string\"}\n",
        "strategy = 'deepspeed_stage_2' #@param ['deepspeed_stage_2', 'deepspeed_stage_2_offload', 'ddp_find_unused_parameters_false'] {type:\"string\"}\n",
        "grad_cp = \"1\" #@param [0, 1] {type:\"string\"}\n",
        "lora_r = 8 #@param {type:\"integer\"}\n",
        "lora_alpha = 16 #@param {type:\"integer\"}\n",
        "lora_dropout = 0.01 #@param {type:\"number\"}\n",
        "\n",
        "# https://github.com/Blealtan/RWKV-LM-LoRA/blob/main/RWKV-v4neo/train.py#L32\n",
        "%cd /content/RWKV-LorA/RWKV-v4neo/\n",
        "if restore_checkpoint == False:\n",
        "  !python train.py \\\n",
        "  --load_model $model_path \\\n",
        "  --proj_dir $checkpoint_dir \\\n",
        "  --data_file $dataset_file \\\n",
        "  --data_type \"binidx\" \\\n",
        "  --vocab_size 50277 \\\n",
        "  --ctx_len $ctx_len \\\n",
        "  --epoch_steps $epoch_steps \\\n",
        "  --epoch_count $epoch_count \\\n",
        "  --epoch_begin $epoch_begin \\\n",
        "  --epoch_save $epoch_save_frequency \\\n",
        "  --micro_bsz $micro_bsz \\\n",
        "  --n_layer $n_layer \\\n",
        "  --n_embd $n_embd \\\n",
        "  --pre_ffn 0 \\\n",
        "  --head_qk 0 \\\n",
        "  --lr_init 1e-5 \\\n",
        "  --lr_final 1e-5 \\\n",
        "  --warmup_steps 0 \\\n",
        "  --beta1 0.9 \\\n",
        "  --beta2 0.999 \\\n",
        "  --adam_eps 1e-8 \\\n",
        "  --accelerator gpu \\\n",
        "  --devices 1 \\\n",
        "  --precision $precision \\\n",
        "  --strategy $strategy \\\n",
        "  --grad_cp $grad_cp \\\n",
        "  --lora \\\n",
        "  --lora_r $lora_r \\\n",
        "  --lora_alpha $lora_alpha \\\n",
        "  --lora_dropout $lora_dropout \\\n",
        "  --lora_parts=att,ffn,time,ln\n",
        "else:\n",
        "  !python train.py \\\n",
        "  --load_model $model_path \\\n",
        "  --proj_dir $checkpoint_dir \\\n",
        "  --data_file $dataset_file \\\n",
        "  --data_type \"binidx\" \\\n",
        "  --vocab_size 50277 \\\n",
        "  --ctx_len $ctx_len \\\n",
        "  --epoch_steps $epoch_steps \\\n",
        "  --epoch_count $epoch_count \\\n",
        "  --epoch_begin $epoch_begin \\\n",
        "  --epoch_save $epoch_save_frequency \\\n",
        "  --micro_bsz $micro_bsz \\\n",
        "  --n_layer $n_layer \\\n",
        "  --n_embd $n_embd \\\n",
        "  --pre_ffn 0 \\\n",
        "  --head_qk 0 \\\n",
        "  --lr_init 1e-5 \\\n",
        "  --lr_final 1e-5 \\\n",
        "  --warmup_steps 0 \\\n",
        "  --beta1 0.9 \\\n",
        "  --beta2 0.999 \\\n",
        "  --adam_eps 1e-8 \\\n",
        "  --accelerator gpu \\\n",
        "  --devices 1 \\\n",
        "  --precision $precision \\\n",
        "  --strategy $strategy \\\n",
        "  --grad_cp $grad_cp \\\n",
        "  --lora \\\n",
        "  --lora_r $lora_r \\\n",
        "  --lora_alpha $lora_alpha \\\n",
        "  --lora_dropout $lora_dropout \\\n",
        "  --lora_load $checkpoint_path\n",
        "\n",
        "# --lora_load <lora checkpoint to continue training> \\ # optional\n",
        "# --lora_parts=att,ffn,time,ln # configure which parts to finetune"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "history_visible": true,
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
